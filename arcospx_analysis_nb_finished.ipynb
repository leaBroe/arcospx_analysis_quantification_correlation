{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T13:44:17.859200Z",
     "start_time": "2023-11-30T13:44:17.573633Z"
    }
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'split'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[8], line 7\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmatplotlib\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpyplot\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mplt\u001B[39;00m\n\u001B[0;32m----> 7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mviewer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Viewer\n\u001B[1;32m      8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mskimage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m io\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/napari-env/lib/python3.10/site-packages/napari/viewer.py:8\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mweakref\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WeakSet\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mmagicgui\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mmgui\u001B[39;00m\n\u001B[0;32m----> 8\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponents\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mviewer_model\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ViewerModel\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _magicgui, config\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;66;03m# helpful for IDE support\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/napari-env/lib/python3.10/site-packages/napari/components/__init__.py:19\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponents\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcamera\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Camera\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponents\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mdims\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Dims\n\u001B[0;32m---> 19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponents\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayerlist\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LayerList\n\u001B[1;32m     21\u001B[0m \u001B[38;5;66;03m# Note that importing _viewer_key_bindings is needed as the Viewer gets\u001B[39;00m\n\u001B[1;32m     22\u001B[0m \u001B[38;5;66;03m# decorated with keybindings during that process, but it is not directly needed\u001B[39;00m\n\u001B[1;32m     23\u001B[0m \u001B[38;5;66;03m# by our users and so is deleted below\u001B[39;00m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcomponents\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _viewer_key_bindings  \u001B[38;5;66;03m# isort:skip\u001B[39;00m\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/napari-env/lib/python3.10/site-packages/napari/components/layerlist.py:9\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m TYPE_CHECKING, Iterable, List, Optional, Tuple, Union\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mnumpy\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mnp\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Layer\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _ImageBase\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevents\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcontainers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SelectableEventedList\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/napari-env/lib/python3.10/site-packages/napari/layers/__init__.py:9\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;124;03m\"\"\"Layers are the viewable objects that can be added to a viewer.\u001B[39;00m\n\u001B[1;32m      2\u001B[0m \n\u001B[1;32m      3\u001B[0m \u001B[38;5;124;03mCustom layers must inherit from Layer and pass along the\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;124;03m`visual node <https://vispy.org/api/vispy.scene.visuals.html>`_\u001B[39;00m\n\u001B[1;32m      5\u001B[0m \u001B[38;5;124;03mto the super constructor.\u001B[39;00m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01minspect\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_inspect\u001B[39;00m\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Layer\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mimage\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Image\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlabels\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Labels\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/napari-env/lib/python3.10/site-packages/napari/layers/base/__init__.py:2\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_base_constants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ActionType\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mbase\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Layer, no_op\n\u001B[1;32m      4\u001B[0m __all__ \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mLayer\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mno_op\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mActionType\u001B[39m\u001B[38;5;124m'\u001B[39m]\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/napari-env/lib/python3.10/site-packages/napari/layers/base/base.py:45\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevents\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevent\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m WarningEmitter\n\u001B[1;32m     41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mgeometry\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m     42\u001B[0m     find_front_back_face,\n\u001B[1;32m     43\u001B[0m     intersect_line_with_axis_aligned_bounding_box_3d,\n\u001B[1;32m     44\u001B[0m )\n\u001B[0;32m---> 45\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkey_bindings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m KeymapProvider\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmouse_bindings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m MousemapProvider\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnaming\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m magic_name\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/napari-env/lib/python3.10/site-packages/napari/utils/key_bindings.py:46\u001B[0m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Callable, Dict\n\u001B[1;32m     44\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mvispy\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keys\n\u001B[0;32m---> 46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msettings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m get_settings\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01maction_manager\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m action_manager\n\u001B[1;32m     48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtranslations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m trans\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/napari-env/lib/python3.10/site-packages/napari/settings/__init__.py:4\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpathlib\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m      2\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Optional\n\u001B[0;32m----> 4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msettings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_base\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _NOT_SET\n\u001B[1;32m      5\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msettings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_napari_settings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m (\n\u001B[1;32m      6\u001B[0m     CURRENT_SCHEMA_VERSION,\n\u001B[1;32m      7\u001B[0m     NapariSettings,\n\u001B[1;32m      8\u001B[0m )\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtranslations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m trans\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/napari-env/lib/python3.10/site-packages/napari/settings/_base.py:16\u001B[0m\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01menv_settings\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SettingsError\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01merror_wrappers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m display_errors\n\u001B[0;32m---> 16\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msettings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_yaml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PydanticYamlMixin\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mevents\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m EmitterGroup, EventedModel\n\u001B[1;32m     18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mmisc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m deep_update\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/napari-env/lib/python3.10/site-packages/napari/settings/_yaml.py:9\u001B[0m\n\u001B[1;32m      6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mpydantic\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m BaseModel\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01myaml\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SafeDumper, dump_all\n\u001B[0;32m----> 9\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msettings\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01m_fields\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Version\n\u001B[1;32m     11\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m TYPE_CHECKING:\n\u001B[1;32m     12\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mcollections\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabc\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Mapping\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/napari-env/lib/python3.10/site-packages/napari/settings/_fields.py:6\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mfunctools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m total_ordering\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Any, Dict, Optional, SupportsInt, Tuple, Union\n\u001B[0;32m----> 6\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtheme\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m available_themes, is_theme_available\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mnapari\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtranslations\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m _load_language, get_language_packs, trans\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28;01mclass\u001B[39;00m \u001B[38;5;21;01mTheme\u001B[39;00m(\u001B[38;5;28mstr\u001B[39m):\n",
      "File \u001B[0;32m~/opt/anaconda3/envs/napari-env/lib/python3.10/site-packages/napari/utils/theme.py:27\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m     25\u001B[0m     \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mqtpy\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m QT_VERSION\n\u001B[0;32m---> 27\u001B[0m     major, minor, \u001B[38;5;241m*\u001B[39m_ \u001B[38;5;241m=\u001B[39m \u001B[43mQT_VERSION\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msplit\u001B[49m(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     28\u001B[0m     use_gradients \u001B[38;5;241m=\u001B[39m (\u001B[38;5;28mint\u001B[39m(major) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m5\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m (\u001B[38;5;28mint\u001B[39m(minor) \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m12\u001B[39m)\n\u001B[1;32m     29\u001B[0m     \u001B[38;5;28;01mdel\u001B[39;00m major, minor, QT_VERSION\n",
      "\u001B[0;31mAttributeError\u001B[0m: 'NoneType' object has no attribute 'split'"
     ]
    }
   ],
   "source": [
    "from arcos4py.tools import track_events_image, remove_image_background\n",
    "from arcos4py.tools._detect_events import upscale_image\n",
    "from arcos4py.tools._cleandata import blockwise_median\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from napari.viewer import Viewer\n",
    "from skimage import io\n",
    "import napari\n",
    "from scipy.ndimage import distance_transform_edt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T13:44:07.253300Z",
     "start_time": "2023-11-30T13:44:07.172919Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.ndimage import binary_dilation, binary_fill_holes, binary_erosion\n",
    "from skimage.morphology import erosion, remove_small_objects, square\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.optimize import curve_fit\n",
    "from skimage import io\n",
    "import numpy as np\n",
    "from skimage import io, exposure\n",
    "\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from skimage.morphology import closing\n",
    "\n",
    "import numpy as np\n",
    "from statsmodels.nonparametric.smoothers_lowess import lowess\n",
    "\n",
    "import numpy as np\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "\n",
    "def drop_scattered_small_labels(label_image, min_size=100):\n",
    "    \"\"\"\n",
    "    Removes small scattered regions of each label from a labeled image.\n",
    "\n",
    "    Parameters:\n",
    "    - label_image: 2D numpy array representing the labeled image.\n",
    "    - min_size: Minimum pixel size for keeping a scattered part of a label.\n",
    "\n",
    "    Returns:\n",
    "    - Processed image with small scattered labels dropped.\n",
    "    \"\"\"\n",
    "    label_image = closing(label_image)\n",
    "    unique_labels = np.unique(label_image)\n",
    "    output_image = np.zeros_like(label_image)\n",
    "\n",
    "    for label in unique_labels:\n",
    "        if label == 0:  # Assuming 0 is the background\n",
    "            continue\n",
    "\n",
    "        # Create a binary image for the current label\n",
    "        binary_mask = label_image == label\n",
    "\n",
    "        # Identify separate regions of the current label\n",
    "        labeled_mask, num_features = ndimage.label(binary_mask)\n",
    "\n",
    "        # Measure the size of each region\n",
    "        sizes = ndimage.sum(binary_mask, labeled_mask, range(num_features + 1))\n",
    "\n",
    "        # Create a mask of regions to be kept for the current label\n",
    "        mask_size = sizes >= min_size\n",
    "        keep = mask_size[labeled_mask]\n",
    "\n",
    "        # Update the output image with regions of the current label that are kept\n",
    "        output_image[keep] = label\n",
    "\n",
    "    return output_image\n",
    "\n",
    "\n",
    "def process_time_series_label_images(time_series_label_images, min_size=100):\n",
    "    \"\"\"\n",
    "    Processes a time-series of label images by removing small scattered labels.\n",
    "\n",
    "    Parameters:\n",
    "    - time_series_label_images: 3D numpy array representing a time-series of labeled images.\n",
    "      The first dimension is time.\n",
    "    - min_size: Minimum pixel size for keeping a scattered part of a label.\n",
    "\n",
    "    Returns:\n",
    "    - Processed time-series with small scattered labels dropped from each frame.\n",
    "    \"\"\"\n",
    "\n",
    "    # Get the number of time points\n",
    "    T = time_series_label_images.shape[0]\n",
    "\n",
    "    # Initialize an output array of the same shape as the input\n",
    "    output_images = np.zeros_like(time_series_label_images)\n",
    "\n",
    "    for t in range(T):\n",
    "        output_images[t] = drop_scattered_small_labels(time_series_label_images[t], min_size=min_size)\n",
    "\n",
    "    return output_images\n",
    "\n",
    "\n",
    "def filter_by_centroid_displacement(labeled_stack, min_distance):\n",
    "    \"\"\"\n",
    "    Removes tracks from a labeled image stack if the total displacement of their centroid\n",
    "    is less than the specified minimum distance.\n",
    "    \"\"\"\n",
    "    labeled_stack = np.copy(labeled_stack)\n",
    "\n",
    "    unique_labels = np.unique(labeled_stack)[1:]  # Exclude background (label 0)\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # Get the coordinates of all pixels belonging to the current label (track) for each time point\n",
    "        time_points = np.unique(np.where(labeled_stack == label)[0])\n",
    "\n",
    "        centroids = []\n",
    "        for t in time_points:\n",
    "            coords = np.argwhere(labeled_stack[t] == label)\n",
    "            centroid = coords.mean(axis=0)\n",
    "            centroids.append(centroid)\n",
    "\n",
    "        # Calculate the total centroid displacement by summing up the distances between consecutive time points\n",
    "        total_distance = sum(np.linalg.norm(centroids[i + 1] - centroids[i]) for i in range(len(centroids) - 1))\n",
    "\n",
    "        # If total displacement is less than min_distance, remove the track\n",
    "        if total_distance < min_distance:\n",
    "            labeled_stack[labeled_stack == label] = 0\n",
    "\n",
    "    return labeled_stack\n",
    "\n",
    "\n",
    "def filter_by_duration(labeled_stack, min_duration):\n",
    "    \"\"\"\n",
    "    Removes tracks from a labeled image stack if their duration is less than the specified minimum.\n",
    "    \"\"\"\n",
    "    labeled_stack = np.copy(labeled_stack)\n",
    "\n",
    "    unique_labels = np.unique(labeled_stack)[1:]  # Exclude background (label 0)\n",
    "\n",
    "    for label in unique_labels:\n",
    "        # Get the coordinates of all pixels belonging to the current label (track) for each time point\n",
    "        time_points = np.unique(np.where(labeled_stack == label)[0])\n",
    "\n",
    "        # If the duration is less than min_duration, remove the track\n",
    "        if len(time_points) < min_duration:\n",
    "            labeled_stack[labeled_stack == label] = 0\n",
    "\n",
    "    return labeled_stack\n",
    "\n",
    "\n",
    "def smooth_segmentation(binary_objects, expand_iterations=1, remove_small=True, remove_small_objects_size=100):\n",
    "    \"\"\"\n",
    "    Smooths the segmentation by removing small objects and filling holes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    binary_objects : numpy array\n",
    "        Binary image of the segmented objects.\n",
    "    remove_small : bool, optional\n",
    "        Whether to remove small objects. The default is True.\n",
    "    remove_small_objects_size : int, optional\n",
    "        Size of the objects to remove. The default is 100.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    binary_objects : numpy array\n",
    "        Smoothed binary image of the segmented objects.\n",
    "    \"\"\"\n",
    "    binary_objects = np.where(binary_objects == 2, 1, 0)\n",
    "    if len(binary_objects.shape) == 3:\n",
    "        for index, image in enumerate(binary_objects):\n",
    "            image = binary_fill_holes(image)\n",
    "            image = binary_dilation(image, square(5), iterations=expand_iterations)\n",
    "            image = erosion(image, footprint=square(5))\n",
    "            bool_img = image.astype(bool)\n",
    "            if remove_small:\n",
    "                image = remove_small_objects(bool_img, min_size=remove_small_objects_size**2)\n",
    "            image = binary_fill_holes(image)\n",
    "            binary = np.where(image, 1, 0)\n",
    "            binary_objects[index] = binary\n",
    "        return binary_objects\n",
    "    else:\n",
    "        binary_objects = binary_fill_holes(binary_objects)\n",
    "        binary_objects = binary_dilation(binary_objects, square(5), iterations=expand_iterations)\n",
    "        binary_objects = erosion(binary_objects, footprint=square(5))\n",
    "        bool_img = binary_objects.astype(bool)\n",
    "        if remove_small:\n",
    "            binary_objects = remove_small_objects(bool_img, min_size=remove_small_objects_size**2)\n",
    "        binary_objects = binary_fill_holes(binary_objects)\n",
    "        binary_objects = np.where(binary_objects, 1, 0)\n",
    "        return binary_objects\n",
    "\n",
    "def bleach_correction_smooth(img_stack, window_length=11, polyorder=2):\n",
    "    \"\"\"\n",
    "    Perform bleach correction on a t,y,x image stack using Savitzky-Golay smoothing.\n",
    "\n",
    "    Parameters:\n",
    "    - img_stack: 3D numpy array with shape (t, y, x)\n",
    "    - window_length: Length of the filter window (must be odd).\n",
    "    - polyorder: Order of the polynomial used to fit the samples.\n",
    "\n",
    "    Returns:\n",
    "    - Corrected 3D numpy array with same shape as img_stack\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert img_stack to float type for the correction\n",
    "    img_stack = img_stack.astype(np.float64)\n",
    "\n",
    "    # Calculate average intensity for each time point\n",
    "    avg_intensities = img_stack.mean(axis=(1, 2))\n",
    "\n",
    "    # Apply Savitzky-Golay filter to average intensities\n",
    "    smoothed_intensities = savgol_filter(avg_intensities, window_length, polyorder)\n",
    "\n",
    "    # Calculate correction factors\n",
    "    correction_factors = smoothed_intensities / avg_intensities[0]\n",
    "\n",
    "    # Apply correction to the image stack\n",
    "    for i in range(img_stack.shape[0]):\n",
    "        img_stack[i] /= correction_factors[i]\n",
    "\n",
    "    return img_stack\n",
    "\n",
    "\n",
    "def bleach_correction_loess(img_stack, frac=0.1):\n",
    "    \"\"\"\n",
    "    Perform bleach correction on a t,y,x image stack using LOESS smoothing.\n",
    "\n",
    "    Parameters:\n",
    "    - img_stack: 3D numpy array with shape (t, y, x)\n",
    "    - frac: The fraction of data used when estimating each y-value for the lowess fit.\n",
    "            It determines the span of the window; for example, a value of 0.1 means\n",
    "            each smoothed point uses 10% of the data points.\n",
    "\n",
    "    Returns:\n",
    "    - Corrected 3D numpy array with same shape as img_stack\n",
    "    \"\"\"\n",
    "\n",
    "    # Convert img_stack to float type for the correction\n",
    "    img_stack = img_stack.astype(np.float64)\n",
    "\n",
    "    # Calculate average intensity for each time point\n",
    "    avg_intensities = img_stack.mean(axis=(1, 2))\n",
    "\n",
    "    # Time points\n",
    "    t_values = np.arange(len(avg_intensities))\n",
    "\n",
    "    # Apply LOESS smoothing to average intensities\n",
    "    smoothed_intensities = lowess(avg_intensities, t_values, frac=frac, return_sorted=False)\n",
    "\n",
    "    # Calculate correction factors\n",
    "    correction_factors = smoothed_intensities / avg_intensities[0]\n",
    "\n",
    "    # Apply correction to the image stack\n",
    "    for i in range(img_stack.shape[0]):\n",
    "        img_stack[i] /= correction_factors[i]\n",
    "\n",
    "    return img_stack\n",
    "\n",
    "\n",
    "def match_histogram(source, template):\n",
    "    hist_source, bin_edges = np.histogram(source.ravel(), bins=65536, range=(0, 65536))\n",
    "    hist_template, _ = np.histogram(template.ravel(), bins=65536, range=(0, 65536))\n",
    "\n",
    "    cdf_source = hist_source.cumsum() / hist_source.sum()\n",
    "    cdf_template = hist_template.cumsum() / hist_template.sum()\n",
    "\n",
    "    lookup_table = np.zeros(65536, dtype=np.uint16)\n",
    "    j = 0\n",
    "    for i in range(65536):\n",
    "        while cdf_template[j] < cdf_source[i] and j < 65535:\n",
    "            j += 1\n",
    "        lookup_table[i] = j\n",
    "\n",
    "    matched = lookup_table[source]\n",
    "    return matched\n",
    "\n",
    "\n",
    "def match_histogram_stack(img_stack, template):\n",
    "    matched_stack = np.zeros_like(img_stack)\n",
    "    for i in range(img_stack.shape[0]):\n",
    "        matched_stack[i] = match_histogram(img_stack[i], template)\n",
    "    return matched_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T13:44:13.384637Z",
     "start_time": "2023-11-30T13:44:09.681512Z"
    }
   },
   "outputs": [],
   "source": [
    "unstable_fibres = io.imread(\"transfer_187559_files_94515bab/lifeact_myosin_rgbd7_w15TIRF-GFP_s1_t1.TIF_-_Stage1__Position_11_.tiff\")\n",
    "stable_fibres = io.imread(\"transfer_187559_files_94515bab/lifeact_myosin_rgbd7_w15TIRF-GFP_s1_t1.TIF_-_Stage2__Position_12_.tiff\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T13:44:13.422038Z",
     "start_time": "2023-11-30T13:44:13.391835Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'napari' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[7], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m viewer \u001B[38;5;241m=\u001B[39m \u001B[43mnapari\u001B[49m\u001B[38;5;241m.\u001B[39mViewer()\n\u001B[1;32m      2\u001B[0m viewer\u001B[38;5;241m.\u001B[39madd_image(unstable_fibres, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munstable fibres\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m      3\u001B[0m viewer\u001B[38;5;241m.\u001B[39madd_image(stable_fibres, name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstable fibres\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[0;31mNameError\u001B[0m: name 'napari' is not defined"
     ]
    }
   ],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(unstable_fibres, name=\"unstable fibres\")\n",
    "viewer.add_image(stable_fibres, name=\"stable fibres\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.646684Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "\n",
    "myosin_bl = []\n",
    "rGBD_bl = []\n",
    "actin_bl = []\n",
    "\n",
    "myosin = stable_fibres[..., 1]\n",
    "rGBD = stable_fibres[..., 2]\n",
    "actin = stable_fibres[..., 0]\n",
    "reference_myosin = myosin[0]\n",
    "reference_rGBD = rGBD[0]\n",
    "reference_actin = actin[0]\n",
    "myosin_bl.append(np.stack([match_histogram(img, reference_myosin) for img in tqdm(myosin)]))\n",
    "rGBD_bl.append(np.stack([match_histogram(img, reference_rGBD) for img in tqdm(rGBD)]))\n",
    "actin_bl.append(np.stack([match_histogram(img, reference_actin) for img in tqdm(actin)]))\n",
    "myosin_bl = np.concatenate(myosin_bl)\n",
    "rGBD_bl = np.concatenate(rGBD_bl)\n",
    "actin_bl = np.concatenate(actin_bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.649566Z"
    }
   },
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "# viewer.add_image(actin)\n",
    "viewer.add_image(actin_bl)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.652202Z"
    }
   },
   "outputs": [],
   "source": [
    "binned_actin = blockwise_median(actin, (1, 3, 3))\n",
    "binned_actin.shape\n",
    "# io.imsave(\"binned_actin.tif\", binned_actin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.653982Z"
    }
   },
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(binned_actin)\n",
    "viewer.add_image(actin_bl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.656007Z"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.morphology import opening\n",
    "from skimage.filters import gaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.657546Z"
    }
   },
   "outputs": [],
   "source": [
    "actin_bg = remove_image_background(binned_actin, size=(100, 1, 1), filter_type=\"median\", crop_time_axis=True)\n",
    "actin_bg = np.where(actin_bg < 0, 0, actin_bg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.659298Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.hist(actin_bg.ravel(), bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T13:43:02.662543Z",
     "start_time": "2023-11-30T13:43:02.661085Z"
    }
   },
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "#viewer.add_image(actin_bg_bl)\n",
    "viewer.add_image(actin_bg)\n",
    "# viewer.add_labels(track_events_image(actin_bg > 3, eps=5, minClSz=15, predictor=True, nPrev=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T13:43:02.674715Z",
     "start_time": "2023-11-30T13:43:02.662800Z"
    }
   },
   "outputs": [],
   "source": [
    "test = gaussian(actin_bg, sigma=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.664342Z"
    }
   },
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(test)\n",
    "viewer.add_image(actin_bg)\n",
    "viewer.add_image(binned_actin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Do seg with convpaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.665867Z"
    }
   },
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(binned_actin)\n",
    "# viewer.add_labels(events_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.667760Z"
    }
   },
   "outputs": [],
   "source": [
    "# viewer = napari.Viewer()\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(test)\n",
    "# viewer.add_image(binned_actin[50:-50])\n",
    "# viewer.add_labels(events_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from cellpose import models"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.669457Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = models.Cellpose(gpu=False, model_type=\"cyto2\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.671303Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "seg_cp = np.zeros_like(binned_actin)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.673568Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-11-30T13:43:02.727159Z",
     "start_time": "2023-11-30T13:43:02.675294Z"
    }
   },
   "outputs": [],
   "source": [
    "for idx, img in enumerate(binned_actin):\n",
    "    masks, flows, styles, diams = model.eval(img, diameter=60, channels=[0, 0], do_3D=False, net_avg=True)\n",
    "    seg_cp[idx] = masks\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.677100Z"
    }
   },
   "outputs": [],
   "source": [
    "seg_cp.astype(np.uint8).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.679155Z"
    }
   },
   "outputs": [],
   "source": [
    "viewer.add_labels(seg_cp.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.681449Z"
    }
   },
   "outputs": [],
   "source": [
    "dist_transform = []\n",
    "for i in seg_cp:\n",
    "    dist_transform.append(distance_transform_edt(i).astype(np.uint16))\n",
    "\n",
    "dist_transform = np.stack(dist_transform)\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(dist_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dist_transform_upscaled = []\n",
    "for t in dist_transform:\n",
    "    t_img = upscale_image(t, 3)\n",
    "    dist_transform_upscaled.append(t_img)\n",
    "\n",
    "dist_transform_upscaled = np.stack(dist_transform_upscaled)\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(dist_transform_upscaled)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.683379Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.685262Z"
    }
   },
   "outputs": [],
   "source": [
    "test_no_edge = np.where((dist_transform[50:-50] < 5), 0, test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.687431Z"
    }
   },
   "outputs": [],
   "source": [
    "viewer.add_image(test_no_edge, name=\"test_no_edge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.689609Z"
    }
   },
   "outputs": [],
   "source": [
    "events_1 = track_events_image(test_no_edge > 40, eps=3, epsPrev=3, minClSz=8, predictor=True, nPrev=3, downsample=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.691326Z"
    }
   },
   "outputs": [],
   "source": [
    "viewer.add_labels(events_1, name=\"events_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.693406Z"
    }
   },
   "outputs": [],
   "source": [
    "events_1_filtered = process_time_series_label_images(events_1, 5)\n",
    "events_1_filtered = filter_by_duration(events_1_filtered, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.695154Z"
    }
   },
   "outputs": [],
   "source": [
    "viewer = napari.current_viewer()\n",
    "viewer.add_labels(events_1, name=\"events_1\")\n",
    "# viewer.add_labels(events_1_filtered, name=\"events_1_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.697800Z"
    }
   },
   "outputs": [],
   "source": [
    "events_upscaled = []\n",
    "for t in events_1_filtered:\n",
    "    t_img = upscale_image(t, 3)\n",
    "    events_upscaled.append(t_img)\n",
    "\n",
    "events_upscaled = np.stack(events_upscaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.699783Z"
    }
   },
   "outputs": [],
   "source": [
    "viewer = napari.current_viewer()\n",
    "viewer.add_labels(events_upscaled, name=\"events_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.701419Z"
    }
   },
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(stable_fibres[50:-50, ...,0 ], name=\"binned_actin\")\n",
    "viewer.add_labels(events_upscaled, name=\"events_1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.703254Z"
    }
   },
   "outputs": [],
   "source": [
    "unstable_fibres.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.705022Z"
    }
   },
   "outputs": [],
   "source": [
    "actin[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.707238Z"
    }
   },
   "outputs": [],
   "source": [
    "from skimage.measure import regionprops_table\n",
    "\n",
    "df_all = []\n",
    "for idx, e in enumerate(events_upscaled):\n",
    "    props = regionprops_table(e, properties=(\"label\", \"area\", \"centroid\", 'intensity_mean'), intensity_image=actin[idx][:, :1160])\n",
    "    df_temp = pd.DataFrame(props)\n",
    "    df_temp[\"frame\"] = idx\n",
    "    df_all.append(df_temp)\n",
    "\n",
    "df_all = pd.concat(df_all)\n",
    "\n",
    "df_all.rename(columns={\"centroid-0\": \"x\", \"centroid-1\": \"y\", \"label\": \"id\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.708605Z"
    }
   },
   "outputs": [],
   "source": [
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "selected_data = df_all.iloc[:12, [2, 3]]\n",
    "selected_data\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.710149Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(stable_fibres[50:-50, ...,0 ], name=\"binned_actin\")\n",
    "viewer.add_labels(events_upscaled, name=\"events_1\")\n",
    "point_layer = viewer.add_points(selected_data, size=5, symbol='cross', name='cross_points')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.711455Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "events_upscaled.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.713218Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def get_summary_statistics(df):\n",
    "    # Initializing lists to hold statistics for all objects\n",
    "    obj_ids = []\n",
    "    origin_x = []\n",
    "    origin_y = []\n",
    "    origin_t = []\n",
    "    end_x = []\n",
    "    end_y = []\n",
    "    end_t = []\n",
    "    lifetimes = []\n",
    "    avg_sizes = []\n",
    "    avg_velocities = []\n",
    "    persistences = []\n",
    "\n",
    "    for obj_id in df[\"id\"].unique():\n",
    "        obj_df = df[df[\"id\"] == obj_id].sort_values(by=\"frame\")\n",
    "\n",
    "        # Origin and End points\n",
    "        origin_x.append(obj_df.iloc[0][\"x\"])\n",
    "        origin_y.append(obj_df.iloc[0][\"y\"])\n",
    "        origin_t.append(obj_df.iloc[0][\"frame\"])\n",
    "        end_x.append(obj_df.iloc[-1][\"x\"])\n",
    "        end_y.append(obj_df.iloc[-1][\"y\"])\n",
    "        end_t.append(obj_df.iloc[-1][\"frame\"])\n",
    "\n",
    "        # Lifetime\n",
    "        lifetime = obj_df[\"frame\"].max() - obj_df[\"frame\"].min() + 1\n",
    "        lifetimes.append(lifetime)\n",
    "\n",
    "        # Average size\n",
    "        avg_sizes.append(obj_df[\"area\"].mean())\n",
    "\n",
    "        # Average velocity\n",
    "        obj_df[\"dx\"] = obj_df[\"x\"].diff()\n",
    "        obj_df[\"dy\"] = obj_df[\"y\"].diff()\n",
    "        obj_df[\"dt\"] = obj_df[\"frame\"].diff()\n",
    "        velocities = np.sqrt(obj_df[\"dx\"] ** 2 + obj_df[\"dy\"] ** 2) / obj_df[\"dt\"]\n",
    "        avg_velocities.append(velocities.mean())\n",
    "\n",
    "        # Persistence\n",
    "        net_displacement = np.linalg.norm(\n",
    "            [obj_df[\"x\"].iloc[-1] - obj_df[\"x\"].iloc[0], obj_df[\"y\"].iloc[-1] - obj_df[\"y\"].iloc[0]]\n",
    "        )\n",
    "        total_distance = np.sum(np.sqrt(obj_df[\"x\"].diff() ** 2 + obj_df[\"y\"].diff() ** 2))\n",
    "        persistence = net_displacement / total_distance if total_distance != 0 else 0  # To avoid division by zero\n",
    "        persistences.append(persistence)\n",
    "\n",
    "        # Add to object IDs list\n",
    "        obj_ids.append(obj_id)\n",
    "\n",
    "    # Creating the summary DataFrame\n",
    "    summary_df = pd.DataFrame(\n",
    "        {\n",
    "            \"id\": obj_ids,\n",
    "            \"origin_x\": origin_x,\n",
    "            \"origin_y\": origin_y,\n",
    "            \"origin_t\": origin_t,\n",
    "            \"end_x\": end_x,\n",
    "            \"end_y\": end_y,\n",
    "            \"end_t\": end_t,\n",
    "            \"lifetime\": lifetimes,\n",
    "            \"avg_size\": avg_sizes,\n",
    "            \"avg_velocity\": avg_velocities,\n",
    "            \"persistence\": persistences,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    return summary_df\n",
    "\n",
    "\n",
    "summary_df = get_summary_statistics(df_all)\n",
    "summary_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.714341Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.715283Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(5, 5), sharex=False)\n",
    "\n",
    "axs = axs.flatten()\n",
    "axs[0].hist(summary_df[\"lifetime\"], bins=10)\n",
    "axs[0].set_title(\"Lifetime [s]\")\n",
    "\n",
    "axs[1].hist(summary_df[\"avg_size\"], bins=10)\n",
    "axs[1].set_title(\"Average Size [px]\")\n",
    "\n",
    "axs[2].hist(summary_df[\"avg_velocity\"], bins=10)\n",
    "axs[2].set_title(\"Average Centorid Velocity \\n [px/s]\")\n",
    "\n",
    "axs[3].hist(summary_df[\"persistence\"], bins=10)\n",
    "axs[3].set_title(\"Persistence [ratio]\")\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.show()\n",
    "plt.savefig(\"summary_stats_fibers.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.716201Z"
    }
   },
   "outputs": [],
   "source": [
    "summary_df.persistence.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.717136Z"
    }
   },
   "outputs": [],
   "source": [
    "# add origin and end points to to napari at their respective frames\n",
    "viewer = napari.current_viewer()\n",
    "viewer.add_points(\n",
    "    summary_df[[\"origin_t\", \"origin_x\", \"origin_y\"]].values,\n",
    "    name=\"origin\",\n",
    "    face_color=\"red\",\n",
    "    symbol=\"cross\",\n",
    "    size=10,\n",
    "    n_dimensional=True,\n",
    ")\n",
    "viewer.add_points(\n",
    "    summary_df[[\"end_t\", \"end_x\", \"end_y\"]].values,\n",
    "    name=\"end\",\n",
    "    face_color=\"blue\",\n",
    "    symbol=\"cross\",\n",
    "    size=10,\n",
    "    n_dimensional=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.718359Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import napari\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(stable_fibres[50:-50, ...,0 ], name=\"binned_actin\")\n",
    "\n",
    "# add origin and end points to to napari at their respective frames\n",
    "viewer = napari.current_viewer()\n",
    "viewer.add_points(\n",
    "    summary_df[[\"origin_t\", \"origin_x\", \"origin_y\"]].values,\n",
    "    name=\"origin\",\n",
    "    face_color=\"red\",\n",
    "    symbol=\"cross\",\n",
    "    size=10,\n",
    "    n_dimensional=True,\n",
    ")\n",
    "viewer.add_points(\n",
    "    summary_df[[\"end_t\", \"end_x\", \"end_y\"]].values,\n",
    "    name=\"end\",\n",
    "    face_color=\"blue\",\n",
    "    symbol=\"cross\",\n",
    "    size=10,\n",
    "    n_dimensional=True,\n",
    ")\n",
    "\n",
    "image_data = viewer.layers['binned_actin'].data\n",
    "\n",
    "#slice_index = image_data.shape[0] // 2\n",
    "selected_slice = image_data[0, :, :]\n",
    "\n",
    "start_points = viewer.layers['origin'].data\n",
    "filtered_starting_points = start_points[12:]\n",
    "end_points = viewer.layers['end'].data\n",
    "filtered_ending_points = end_points[12:]\n",
    "\n",
    "\n",
    "x_points_start = [point[2] for point in filtered_starting_points]\n",
    "y_points_start = [point[1] for point in filtered_starting_points]\n",
    "\n",
    "x_points_end = [point[2] for point in filtered_ending_points]\n",
    "y_points_end = [point[1] for point in filtered_ending_points]\n",
    "\n",
    "plt.scatter(x_points_start, y_points_start, c='red', s=10) \n",
    "plt.scatter(x_points_end, y_points_end, c='blue', s=10)\n",
    "plt.imshow(selected_slice, cmap='gray')  \n",
    "#plt.legend(['Starting points', 'Ending points'], loc='upper right')\n",
    "plt.tight_layout()\n",
    "plt.title(f'Starting and ending points')\n",
    "plt.axis('off')  \n",
    "plt.savefig('starting_and_ending_points.png')\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.719395Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "filtered_df_id_1 = df_all[df_all['id'] == 1]\n",
    "filtered_df_id_1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.720527Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import napari\n",
    "import pandas as pd\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(stable_fibres[50:-50, ...,0 ], name=\"binned_actin\")\n",
    "\n",
    "# add origin and end points to to napari at their respective frames\n",
    "viewer = napari.current_viewer()\n",
    "viewer.add_points(\n",
    "    summary_df[[\"origin_t\", \"origin_x\", \"origin_y\"]].values,\n",
    "    name=\"origin\",\n",
    "    face_color=\"red\",\n",
    "    symbol=\"cross\",\n",
    "    size=10,\n",
    "    n_dimensional=True,\n",
    ")\n",
    "viewer.add_points(\n",
    "    summary_df[[\"end_t\", \"end_x\", \"end_y\"]].values,\n",
    "    name=\"end\",\n",
    "    face_color=\"blue\",\n",
    "    symbol=\"cross\",\n",
    "    size=10,\n",
    "    n_dimensional=True,\n",
    ")\n",
    "\n",
    "image_data = viewer.layers['binned_actin'].data\n",
    "\n",
    "#slice_index = image_data.shape[0] // 2\n",
    "selected_slice = image_data[0, :, :]\n",
    "\n",
    "start_points = viewer.layers['origin'].data\n",
    "#start_points = start_points[12:]\n",
    "# change the column names to match the summary_df\n",
    "start_points = pd.DataFrame(start_points, columns=['origin_t', 'origin_x', 'origin_y'])\n",
    "merged_df_start = pd.merge(start_points, summary_df[['id', 'origin_x', 'origin_y']], on=['origin_x', 'origin_y'], how='left')\n",
    "\n",
    "end_points = viewer.layers['end'].data\n",
    "end_points = pd.DataFrame(end_points, columns=['end_t', 'end_x', 'end_y'])\n",
    "merged_df_end = pd.merge(end_points, summary_df[['id', 'end_x', 'end_y']], on=['end_x', 'end_y'], how='left')\n",
    "\n",
    "# Merge the DataFrames on 'id'\n",
    "merged_df_full = pd.merge(merged_df_start, merged_df_end, on='id', suffixes=('_start', '_end'))\n",
    "\n",
    "# Calculate differences (dx, dy) for arrow directions\n",
    "merged_df_full['dx'] = merged_df_full['end_x'] - merged_df_full['origin_x']\n",
    "merged_df_full['dy'] = merged_df_full['end_y'] - merged_df_full['origin_y']\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.quiver(merged_df_full['origin_y'], merged_df_full['origin_x'], merged_df_full['dy'], merged_df_full['dx'], angles='xy', scale_units='xy', scale=1, color='green', width=0.003)\n",
    "\n",
    "# Optionally add start and end points\n",
    "plt.scatter(merged_df_full['origin_y'], merged_df_full['origin_x'], color='red', label='Start Points', s=10)\n",
    "plt.scatter(merged_df_full['end_y'], merged_df_full['end_x'], color='blue', label='End Points', s=10)\n",
    "plt.imshow(selected_slice, cmap='gray')  \n",
    "\n",
    "plt.xlabel('X Coordinate')\n",
    "plt.ylabel('Y Coordinate')\n",
    "plt.legend()\n",
    "plt.axis('off')\n",
    "#plt.savefig('arrows_start_end.png',  dpi=300)\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.721723Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Extract Values from the Distance Transformation Map"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def extract_value(x, y, dist_transform_upscaled, slice_index=0):\n",
    "    # Convert coordinates to integers\n",
    "    x = int(x)\n",
    "    y = int(y)\n",
    "\n",
    "    # Extract value from the distance map\n",
    "    return dist_transform_upscaled[slice_index, x, y]\n",
    "\n",
    "# Apply the function to each row for origin and end points\n",
    "merged_df_full['origin_dist_value'] = merged_df_full.apply(lambda row: extract_value(row['origin_x'], row['origin_y'], dist_transform_upscaled), axis=1)\n",
    "merged_df_full['end_dist_value'] = merged_df_full.apply(lambda row: extract_value(row['end_x'], row['end_y'], dist_transform_upscaled), axis=1)\n",
    "\n",
    "mean_origin_dist_value = np.mean(merged_df_full[\"origin_dist_value\"])\n",
    "mean_end_dist_value = np.mean(merged_df_full[\"end_dist_value\"])\n",
    "\n",
    "print(f\"Mean origin distance value: {mean_origin_dist_value}\")\n",
    "print(f\"Mean end distance value: {mean_end_dist_value}\")\n",
    "\n",
    "#print(dist_transform_upscaled[0, 68, 416])\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.722619Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# check if data is normally distributed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Perform the Shapiro-Wilk test\n",
    "#shapiro_test = stats.shapiro(merged_df_full['origin_dist_value'])\n",
    "shapiro_test = stats.shapiro(merged_df_full['end_dist_value'])\n",
    "\n",
    "\n",
    "print(\"Shapiro-Wilk Test statistic:\", shapiro_test.statistic)\n",
    "print(\"Shapiro-Wilk Test p-value:\", shapiro_test.pvalue)\n",
    "\n",
    "if shapiro_test.pvalue > 0.05:\n",
    "    print(\"Data appears to be normally distributed\")\n",
    "else:\n",
    "    print(\"Data does not appear to be normally distributed\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.723635Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "axs[0, 0].hist(merged_df_full['end_dist_value'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axs[0, 0].set_title('Histogram of end_dist_value')\n",
    "axs[0, 0].set_xlabel('Value')\n",
    "axs[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "axs[0, 1].hist(merged_df_full['origin_dist_value'], bins=30, edgecolor='black', alpha=0.7)\n",
    "axs[0, 1].set_title('Histogram of origin_dist_value')\n",
    "axs[0, 1].set_xlabel('Value')\n",
    "axs[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "stats.probplot(merged_df_full['end_dist_value'], dist=\"norm\", plot=axs[1, 0])\n",
    "axs[1, 0].set_title('Q-Q Plot of end_dist_value')\n",
    "\n",
    "stats.probplot(merged_df_full['origin_dist_value'], dist=\"norm\", plot=axs[1, 1])\n",
    "axs[1, 1].set_title('Q-Q Plot of origin_dist_value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.724756Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Make log transformation"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "\n",
    "merged_df_full['origin_dist_value'] += 1\n",
    "merged_df_full['end_dist_value'] += 1\n",
    "\n",
    "# Perform log transformation\n",
    "merged_df_full['log_origin_dist_value'] = np.log(merged_df_full['origin_dist_value'])\n",
    "merged_df_full['log_end_dist_value'] = np.log(merged_df_full['end_dist_value'])\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(10, 10))\n",
    "\n",
    "axs[0, 0].hist(merged_df_full['log_origin_dist_value'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axs[0, 0].set_title('Histogram of log_origin_dist_value')\n",
    "axs[0, 0].set_xlabel('Value')\n",
    "axs[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "axs[0, 1].hist(merged_df_full['log_end_dist_value'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axs[0, 1].set_title('Histogram of log_end_dist_value')\n",
    "axs[0, 1].set_xlabel('Value')\n",
    "axs[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "stats.probplot(merged_df_full['log_origin_dist_value'], dist=\"norm\", plot=axs[1, 0])\n",
    "axs[1, 0].set_title('Q-Q Plot of log_origin_dist_value')\n",
    "\n",
    "stats.probplot(merged_df_full['log_end_dist_value'], dist=\"norm\", plot=axs[1, 1])\n",
    "axs[1, 1].set_title('Q-Q Plot of log_end_dist_value')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.726340Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import seaborn\n",
    "\n",
    "merged_df_full[\"delta_start_end\"] = merged_df_full[\"origin_dist_value\"] - merged_df_full[\"end_dist_value\"]\n",
    "\n",
    "print(merged_df_full[\"delta_start_end\"].describe())\n",
    "\n",
    "# plot density plot\n",
    "merged_df_full[\"delta_start_end\"].plot(kind=\"density\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-30T13:43:02.744810Z",
     "start_time": "2023-11-30T13:43:02.727526Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# make statistical test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Wilcoxon Signed-Rank Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.stats import wilcoxon\n",
    "\n",
    "# If data is normally distributed, use paired t-test\n",
    "# t_statistic, p_value = ttest_rel(merged_df_full['origin_dist_value'], merged_df_full['end_dist_value'])\n",
    "# print(\"Paired t-test results: t-statistic =\", t_statistic, \", p-value =\", p_value)\n",
    "\n",
    "# since data is not normally distributed, use Wilcoxon signed-rank test\n",
    "# w_statistic, p_value = wilcoxon(merged_df_full['origin_dist_value'], merged_df_full['end_dist_value'])\n",
    "start_end_differences = merged_df_full['delta_start_end'].to_numpy()\n",
    "wilcoxon_test = wilcoxon(start_end_differences)\n",
    "w_statistic, p_value = wilcoxon_test\n",
    "#print(len(start_end_differences))\n",
    "print(\"Wilcoxon statistic =\", w_statistic, \", p-value =\", p_value)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.728513Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "\n",
    "differences = merged_df_full['delta_start_end'].to_numpy()\n",
    "\n",
    "non_zero_differences = np.array([d for d in differences if d != 0])\n",
    "ranks = np.argsort(np.argsort(np.abs(non_zero_differences))) + 1\n",
    "\n",
    "# Assign signs to ranks\n",
    "signed_ranks = ranks * np.sign(non_zero_differences)\n",
    "\n",
    "# Calculate sum of positive and negative ranks\n",
    "W_pos = np.sum(signed_ranks[signed_ranks > 0])\n",
    "W_neg = np.abs(np.sum(signed_ranks[signed_ranks < 0]))\n",
    "\n",
    "# Test statistic is the smaller of W_pos and W_neg\n",
    "W = min(W_pos, W_neg)\n",
    "\n",
    "# For large samples, approximate the p-value using a normal distribution\n",
    "n = len(non_zero_differences)\n",
    "mean_W = n * (n + 1) / 4\n",
    "std_W = np.sqrt(n * (n + 1) * (2 * n + 1) / 24)\n",
    "z = (W - mean_W) / std_W\n",
    "p_value = 2 * norm.cdf(z)\n",
    "\n",
    "print(\"Wilcoxon Signed-Rank Test statistic:\", W)\n",
    "print(\"P-value:\", p_value)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.729637Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Correlation of starting Distance to border and magnitude of differences"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "# Assuming merged_df_full['origin_dist_value'] and merged_df_full['delta_start_end'] are defined\n",
    "x = merged_df_full['origin_dist_value']\n",
    "y = merged_df_full['delta_start_end']\n",
    "\n",
    "# Perform linear regression\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "# Calculate R^2 value\n",
    "r_squared = r_value**2\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Starting distance to border of cell')\n",
    "plt.ylabel('Difference in start and end points')\n",
    "plt.title(\"Correlation of starting distance to border and magnitude of differences\")\n",
    "\n",
    "# Plot the regression line\n",
    "regression_line = slope * x + intercept\n",
    "plt.plot(x, regression_line, color='red')  # You can change the color if needed\n",
    "\n",
    "# Annotate the plot with R^2 and p-value\n",
    "plt.annotate(f'R^2 = {r_squared:.3f}', xy=(0.05, 0.95), xycoords='axes fraction')\n",
    "plt.annotate(f'p-value = {p_value:.3g}', xy=(0.05, 0.90), xycoords='axes fraction')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.730597Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "x = merged_df_full['end_dist_value']\n",
    "y = merged_df_full['delta_start_end']\n",
    "\n",
    "# Perform linear regression\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "# Calculate R^2 value\n",
    "r_squared = r_value**2\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Ending distance to border of cell')\n",
    "plt.ylabel('Difference in start and end points')\n",
    "plt.title(\"Correlation of ending distance to border and magnitude of differences\")\n",
    "\n",
    "# Plot the regression line\n",
    "regression_line = slope * x + intercept\n",
    "plt.plot(x, regression_line, color='red')  # You can change the color if needed\n",
    "\n",
    "# Annotate the plot with R^2 and p-value\n",
    "plt.annotate(f'R^2 = {r_squared:.3f}', xy=(0.05, 0.95), xycoords='axes fraction')\n",
    "plt.annotate(f'p-value = {p_value:.3g}', xy=(0.05, 0.90), xycoords='axes fraction')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.731557Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "x = merged_df_full['end_dist_value']\n",
    "y = merged_df_full['origin_dist_value']\n",
    "\n",
    "# Perform linear regression\n",
    "slope, intercept, r_value, p_value, std_err = stats.linregress(x, y)\n",
    "\n",
    "# Calculate R^2 value\n",
    "r_squared = r_value**2\n",
    "\n",
    "# Scatter plot\n",
    "plt.scatter(x, y)\n",
    "plt.xlabel('Ending distance to border of cell')\n",
    "plt.ylabel('Starting distance to border of cell')\n",
    "plt.title(\"\")\n",
    "\n",
    "# Plot the regression line\n",
    "regression_line = slope * x + intercept\n",
    "plt.plot(x, regression_line, color='red')  # You can change the color if needed\n",
    "\n",
    "# Annotate the plot with R^2 and p-value\n",
    "plt.annotate(f'R^2 = {r_squared:.3f}', xy=(0.05, 0.95), xycoords='axes fraction')\n",
    "plt.annotate(f'p-value = {p_value:.3g}', xy=(0.05, 0.90), xycoords='axes fraction')\n",
    "\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.732496Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sign Test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import statsmodels\n",
    "from scipy.stats import binom\n",
    "\n",
    "start_end_differences = merged_df_full['delta_start_end'].to_numpy()\n",
    "\n",
    "# Remove zeros and count positive and negative differences\n",
    "non_zero_differences = [d for d in start_end_differences if d != 0]\n",
    "num_positive = sum(1 for d in non_zero_differences if d > 0)\n",
    "num_negative = len(non_zero_differences) - num_positive\n",
    "\n",
    "# Calculate the test statistic (the smaller of num_positive and num_negative)\n",
    "test_statistic = min(num_positive, num_negative)\n",
    "\n",
    "# Total number of non-zero observations\n",
    "n = len(non_zero_differences)\n",
    "\n",
    "# Calculate the p-value (two-tailed test)\n",
    "p_value = 2 * binom.cdf(test_statistic, n, 0.5)\n",
    "\n",
    "print(\"Sign test statistic:\", test_statistic)\n",
    "print(\"P-value:\", p_value)\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.733409Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with 1 row and 2 columns\n",
    "fig, axs = plt.subplots(1, 2, figsize=(10, 5))  # Adjusted figsize for better aspect ratio\n",
    "\n",
    "# Plot on the first subplot\n",
    "axs[0].hist(merged_df_full['delta_start_end'], bins=20, edgecolor='black', alpha=0.7)\n",
    "axs[0].set_title('Histogram of delta_start_end')\n",
    "axs[0].set_xlabel('Value')\n",
    "axs[0].set_ylabel('Frequency')\n",
    "\n",
    "# Plot on the second subplot\n",
    "stats.probplot(merged_df_full['delta_start_end'], dist=\"norm\", plot=axs[1])\n",
    "axs[1].set_title('QQ-plot of delta_start_end')\n",
    "axs[1].set_xlabel('Value')\n",
    "axs[1].set_ylabel('Frequency')\n",
    "\n",
    "# Adjust layout and display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.734308Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Labeling of individual IDs"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Create boolean mask for ID = 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import napari\n",
    "\n",
    "segmented_data = events_upscaled\n",
    "\n",
    "specific_id = 1\n",
    "\n",
    "# Create boolean mask\n",
    "boolean_mask = segmented_data == specific_id\n",
    "\n",
    "# convert to int \n",
    "#int_mask = boolean_mask.astype(int)\n",
    "\n",
    "viewer.add_labels(boolean_mask, name='boolean_mask_layer')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.735342Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## label boolean mask from ID = 1 into individual segments"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# start the viewer\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(stable_fibres[50:-50, ...,0 ], name=\"binned_actin\")\n",
    "viewer.add_labels(events_upscaled, name=\"events_1\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.736777Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from scipy.ndimage import label\n",
    "\n",
    "# 'labeled_results' will be a list of tuples, where each tuple contains (labeled_array, num_features) for each time step\n",
    "labeled_results = [label(frame) for frame in boolean_mask]\n",
    "labeled_arrays = [result[0] for result in labeled_results]\n",
    "num_features = [result[1] for result in labeled_results]\n",
    "\n",
    "# Stack the labeled arrays along a dimension\n",
    "stacked_array_id_1 = np.stack(labeled_arrays, axis=0)\n",
    "# 'stacked_array' is now a 3D numpy array where each 'slice' along the third dimension is a labeled time step\n",
    "print(max(num_features))   \n",
    "#labeled_array, num_features = stacked_array\n",
    "\n",
    "#print('Number of features:', num_features)\n",
    "viewer.add_labels(stacked_array_id_1, name='labelled_mask_layer_ID_1')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.737990Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Create regionprops table for individual ID segments (here ID = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from skimage.measure import regionprops_table\n",
    "\n",
    "df_all_id_1 = []\n",
    "for idx, e in enumerate(stacked_array_id_1):\n",
    "    props = regionprops_table(e, properties=(\"label\", \"area\", \"centroid\", 'intensity_mean'), intensity_image=actin[idx][:, :1160])\n",
    "    df_temp = pd.DataFrame(props)\n",
    "    df_temp[\"frame\"] = idx\n",
    "    df_all_id_1.append(df_temp)\n",
    "\n",
    "df_all_id_1 = pd.concat(df_all_id_1)\n",
    "\n",
    "df_all_id_1.rename(columns={\"centroid-0\": \"x\", \"centroid-1\": \"y\", \"label\": \"id\"}, inplace=True)\n",
    "df_all_id_1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.738963Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from trackpy import link_df\n",
    "\n",
    "trackpy_df_id_1 = pd.DataFrame(df_all_id_1[['x', 'y', 'frame']])\n",
    "\n",
    "linked_df_id_1 = link_df(trackpy_df_id_1, search_range=10, verify_integrity=True)\n",
    "\n",
    "linked_df_id_1\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.740259Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.741648Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "\n",
    "class CameraSetter:\n",
    "    \"\"\"A context manager to adjust viewer camera settings before rendering.\"\"\"\n",
    "\n",
    "    def __init__(self, viewer):\n",
    "        self.viewer = viewer\n",
    "        # get initial settings\n",
    "        self.center = viewer.camera.center\n",
    "        self.zoom = viewer.camera.zoom\n",
    "        self.angles = viewer.camera.angles\n",
    "\n",
    "        self.input_canvas_size = viewer.window.qt_viewer.canvas.size\n",
    "\n",
    "        extent = viewer._sliced_extent_world[:, -2:]\n",
    "        scene_size = (extent[1] - extent[0]) / viewer.window.qt_viewer.canvas.pixel_scale  # adjust for pixel scaling\n",
    "        grid_size = list(viewer.grid.actual_shape(len(viewer.layers)))\n",
    "\n",
    "        # Adjust grid_size if necessary\n",
    "        if len(scene_size) > len(grid_size):\n",
    "            grid_size = [1] * (len(scene_size) - len(grid_size)) + grid_size\n",
    "\n",
    "        # calculate target size i.e the size the canvas should be to fit the whole scene\n",
    "        self.target_size = tuple((scene_size[::-1] / 2 * grid_size[::-1]).astype(int))\n",
    "        self.center = viewer.camera.center\n",
    "        self.zoom = viewer.camera.zoom\n",
    "        self.angles = viewer.camera.angles\n",
    "\n",
    "    # copied from viewer.reset_view and modified without padding\n",
    "    def _center_on_canvas(self):\n",
    "        \"\"\"Reset the camera view.\"\"\"\n",
    "        extent = self.viewer._sliced_extent_world\n",
    "        scene_size = extent[1] - extent[0]\n",
    "        corner = extent[0]\n",
    "        grid_size = list(self.viewer.grid.actual_shape(len(self.viewer.layers)))\n",
    "        if len(scene_size) > len(grid_size):\n",
    "            grid_size = [1] * (len(scene_size) - len(grid_size)) + grid_size\n",
    "        size = np.multiply(scene_size, grid_size)\n",
    "        center = np.add(corner, np.divide(size, 2))[-self.viewer.dims.ndisplay :]\n",
    "        center = [0] * (self.viewer.dims.ndisplay - len(center)) + list(center)\n",
    "        self.viewer.camera.center = center\n",
    "\n",
    "        if np.max(size) == 0:\n",
    "            self.viewer.camera.zoom = np.min(self.viewer._canvas_size)\n",
    "        else:\n",
    "            scale = np.array(size[-2:])\n",
    "            scale[np.isclose(scale, 0)] = 1\n",
    "            self.viewer.camera.zoom = 1 * np.min(np.array(self.viewer._canvas_size) / scale)\n",
    "        self.viewer.camera.angles = (0, 0, 90)\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Set up the viewer for rendering.\"\"\"\n",
    "        self.viewer.window.qt_viewer.canvas.size = self.target_size\n",
    "        self._center_on_canvas()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Reset the viewer after rendering.\"\"\"\n",
    "        self.viewer.window.qt_viewer.canvas.size = self.input_canvas_size\n",
    "        self.viewer.camera.center = self.center\n",
    "        self.viewer.camera.zoom = self.zoom\n",
    "        self.viewer.camera.angles = self.angles\n",
    "\n",
    "\n",
    "def render_as_rgb(viewer, axis: None | int = None):\n",
    "    \"\"\"Render the viewer for a single timepoint.\"\"\"\n",
    "    with CameraSetter(viewer):\n",
    "        if axis is not None:\n",
    "            rgb = []\n",
    "            for i in range(viewer.dims.range[axis][1].astype(int)):\n",
    "                viewer.dims.set_current_step(axis, i)\n",
    "                rendered_img = viewer.window.qt_viewer.canvas.render(alpha=False)\n",
    "                rgb.append(rendered_img)\n",
    "            rendered_img = np.stack(rgb)\n",
    "        else:\n",
    "            rendered_img = viewer.window.qt_viewer.canvas.render(alpha=False)\n",
    "    return rendered_img\n",
    "\n",
    "\n",
    "# Usage\n",
    "rgb = []\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    rendered_img = render_as_rgb(viewer, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.742577Z"
    }
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "# Read the first image to get the width, height\n",
    "frame = rendered_img[0]\n",
    "h, w, layers = frame.shape\n",
    "\n",
    "# Define the codec and create a VideoWriter object\n",
    "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "out = cv2.VideoWriter(\"tracked_waves_actin_2.mp4\", fourcc, 60, (w, h))\n",
    "\n",
    "for image in rendered_img:\n",
    "    out.write(cv2.cvtColor(image, cv2.COLOR_RGBA2BGR))  # Write out frame to video\n",
    "\n",
    "# Release everything when the job is finished\n",
    "out.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.743545Z"
    }
   },
   "outputs": [],
   "source": [
    "viewer = napari.Viewer()\n",
    "viewer.add_image(actin, colormap=\"gray_r\", name=\"actin\")\n",
    "viewer.add_image(myosin, colormap=\"gray_r\", name=\"myosin\")\n",
    "viewer.add_image(rGBD, colormap=\"gray_r\", name=\"rGBD\")\n",
    "viewer.add_labels(events_upscaled, name=\"events_filtered\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.744477Z"
    }
   },
   "outputs": [],
   "source": [
    "io.imshow(rendered_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.745423Z"
    }
   },
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import napari\n",
    "import numpy as np\n",
    "import warnings\n",
    "from skimage import io\n",
    "from magicgui import magicgui\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "\n",
    "class CameraSetter:\n",
    "    \"\"\"A context manager to adjust viewer camera settings before rendering.\"\"\"\n",
    "\n",
    "    def __init__(self, viewer, upsample_factor=1):\n",
    "        self.viewer = viewer\n",
    "        # get initial settings\n",
    "        self.center = viewer.camera.center\n",
    "        self.zoom = viewer.camera.zoom\n",
    "        self.angles = viewer.camera.angles\n",
    "\n",
    "        self.input_canvas_size = viewer.window.qt_viewer.canvas.size\n",
    "\n",
    "        extent = viewer._sliced_extent_world[:, -2:]\n",
    "        scene_size = (extent[1] - extent[0]) / viewer.window.qt_viewer.canvas.pixel_scale * upsample_factor # adjust for pixel scaling\n",
    "        grid_size = list(viewer.grid.actual_shape(len(viewer.layers)))\n",
    "\n",
    "        # Adjust grid_size if necessary\n",
    "        if len(scene_size) > len(grid_size):\n",
    "            grid_size = [1] * (len(scene_size) - len(grid_size)) + grid_size\n",
    "\n",
    "        # calculate target size i.e the size the canvas should be to fit the whole scene\n",
    "        self.target_size = tuple((scene_size[::-1] * grid_size[::-1]).astype(int))\n",
    "        self.center = viewer.camera.center\n",
    "        self.zoom = viewer.camera.zoom\n",
    "        self.angles = viewer.camera.angles\n",
    "\n",
    "    # copied from viewer.reset_view and modified without padding\n",
    "    def _center_on_canvas(self):\n",
    "        \"\"\"Reset the camera view.\"\"\"\n",
    "        extent = self.viewer._sliced_extent_world\n",
    "        scene_size = extent[1] - extent[0]\n",
    "        corner = extent[0]\n",
    "        grid_size = list(self.viewer.grid.actual_shape(len(self.viewer.layers)))\n",
    "        if len(scene_size) > len(grid_size):\n",
    "            grid_size = [1] * (len(scene_size) - len(grid_size)) + grid_size\n",
    "        size = np.multiply(scene_size, grid_size)\n",
    "        center = np.add(corner, np.divide(size, 2))[-self.viewer.dims.ndisplay :]\n",
    "        center = [0] * (self.viewer.dims.ndisplay - len(center)) + list(center)\n",
    "        self.viewer.camera.center = center\n",
    "\n",
    "        if np.max(size) == 0:\n",
    "            self.viewer.camera.zoom = np.min(self.viewer._canvas_size)\n",
    "        else:\n",
    "            scale = np.array(size[-2:])\n",
    "            scale[np.isclose(scale, 0)] = 1\n",
    "            self.viewer.camera.zoom = 1 * np.min(np.array(self.viewer._canvas_size) / scale)\n",
    "        self.viewer.camera.angles = (0, 0, 90)\n",
    "\n",
    "    def __enter__(self):\n",
    "        \"\"\"Set up the viewer for rendering.\"\"\"\n",
    "        self.viewer.window.qt_viewer.canvas.size = self.target_size\n",
    "        self._center_on_canvas()\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        \"\"\"Reset the viewer after rendering.\"\"\"\n",
    "        self.viewer.window.qt_viewer.canvas.size = self.input_canvas_size\n",
    "        self.viewer.camera.center = self.center\n",
    "        self.viewer.camera.zoom = self.zoom\n",
    "        self.viewer.camera.angles = self.angles\n",
    "\n",
    "def get_choices(gui):\n",
    "    \"\"\"Return the choices for the axis dropdown.\"\"\"\n",
    "    viewer: napari.Viewer\n",
    "    viewer = napari.current_viewer()\n",
    "    choices = []\n",
    "    for i, axis in enumerate(viewer.dims.axis_labels[:-2]):\n",
    "        if axis is not None:\n",
    "            choices.append(i)\n",
    "    return choices\n",
    "\n",
    "@magicgui(call_button=\"export\", directory={\"mode\": \"d\", \"label\": \"Choose a directory\"}, axis={\"choices\": get_choices}, output_type={\"choices\": [\"tif\", \"mp4\"]})\n",
    "def render_as_rgb(viewer: napari.Viewer, axis: Optional[int], directory=Path(), name: str = \"out\",  output_type = \"mp4\", fps: int = 12, upsample_factor: int = 1):\n",
    "    \"\"\"Render the viewer for a single timepoint.\"\"\"\n",
    "    axis = int(axis) if axis is not None else None\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        with CameraSetter(viewer, upsample_factor):\n",
    "            if axis is not None:\n",
    "                rgb = []\n",
    "                for i in range(viewer.dims.range[axis][1].astype(int)):\n",
    "                    viewer.dims.set_current_step(axis, i)\n",
    "                    rendered_img = viewer.window.qt_viewer.canvas.render(alpha=False)\n",
    "                    rgb.append(rendered_img)\n",
    "                rendered_img = np.stack(rgb)\n",
    "            else:\n",
    "                rendered_img = viewer.window.qt_viewer.canvas.render(alpha=False)\n",
    "\n",
    "    outpath = directory.joinpath(f\"{name}.{output_type}\").as_posix()\n",
    "    if output_type == \"tif\":\n",
    "        io.imsave(outpath, rendered_img)\n",
    "    elif output_type == \"mp4\":\n",
    "        if axis is None:\n",
    "            raise ValueError(\"You must specify an axis to export as mp4\")\n",
    "        try:\n",
    "            import cv2\n",
    "        except ImportError:\n",
    "            raise ImportError(\"You must install opencv to export as mp4, try `pip install opencv-python`\")\n",
    "\n",
    "        # Read the first image to get the width, height\n",
    "        frame = rendered_img[0]\n",
    "        h, w, layers = frame.shape\n",
    "\n",
    "        # Define the codec and create a VideoWriter object\n",
    "        fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "        out = cv2.VideoWriter(outpath, fourcc, fps, (w, h))\n",
    "\n",
    "        for image in rendered_img:\n",
    "            out.write(cv2.cvtColor(image, cv2.COLOR_RGBA2BGR))  # Write out frame to video\n",
    "\n",
    "        # Release everything when the job is finished\n",
    "        out.release()\n",
    "        try:\n",
    "            cv2.destroyAllWindows()\n",
    "        except:\n",
    "            print(\"could not close cv2 windows\")\n",
    "\n",
    "\n",
    "if globals().get(\"viewer\") is None:\n",
    "    viewer = napari.Viewer()\n",
    "else:\n",
    "    viewer = globals()[\"viewer\"]\n",
    "\n",
    "viewer.window.add_dock_widget(render_as_rgb, name=\"render as rgb\")\n",
    "viewer.show(block=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-11-30T13:43:02.746425Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "imageanalysis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
